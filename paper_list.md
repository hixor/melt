# Papers

## Loss surfaces and optimization methods

- Choromanska, Anna, et al. ["The loss surfaces of multilayer networks."](https://arxiv.org/abs/1412.0233) *Artificial Intelligence and Statistics.* 2015.
- Dauphin, Yann N., et al. ["Identifying and attacking the saddle point problem in high-dimensional non-convex optimization."](https://arxiv.org/abs/1406.2572) *Advances in neural information processing systems.* 2014.

### Escaping saddle points

- Jin, Chi, et al. ["How to escape saddle points efficiently."](https://arxiv.org/abs/1703.00887) *Proceedings of the 34th International Conference on Machine Learning-Volume 70*. JMLR. org, 2017.
- Jin, Chi, et al. ["Stochastic Gradient Descent Escapes Saddle Points Efficiently."](https://arxiv.org/abs/1902.04811) *arXiv preprint arXiv:1902.04811*. 2019.

### Elastic Band
- Draxler, Felix, et al. ["Essentially No Barriers in Neural Network Energy Landscape."](http://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf) *Proceedings of the 35th International Conference on Machine Learning*, in PMLR 80:1309-1318. 2018

### Renormalization group 
- Mehta, Pankaj, and David J. Schwab. ["An exact mapping between the variational renormalization group and deep learning."](https://arxiv.org/abs/1410.3831) *arXiv preprint arXiv:1410.3831* 2014.


## Model Aggregation

- Cheung, Brian, et al. ["Superposition of many models into one."](https://arxiv.org/abs/1902.05522) *arXiv preprint arXiv:1902.05522*. 2019.

### Distillation
- Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean.[ "Distilling the knowledge in a neural network."](https://arxiv.org/abs/1503.02531) *arXiv preprint arXiv:1503.02531* 2015.

### Stochastic Weight Averaging
- Izmailov, P., et al.[ "Averaging weights leads to wider optima and better generalization. arXiv 2018."](https://arxiv.org/abs/1803.05407) *arXiv preprint arXiv:1803.05407: 876-885*. 2018

### Fast Geometric Ensembling
- Garipov, Timur, et al. ["Loss surfaces, mode connectivity, and fast ensembling of dnns."](https://arxiv.org/abs/1802.10026) *Advances in Neural Information Processing Systems*. 2018.
	- 	local optima are not isolated, but connected by simple curves
	-  proposes new method to discover paths between optima along which loss is constant - i.e., finds level sets
	-  can ensemble models along the curves

### "Structured dropout"
- Wang, Chaoqi, et al. ["EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis."](https://arxiv.org/abs/1905.05934) *arXiv preprint arXiv:1905.05934*. 2019.


## Characterizing neural nets

### CKA ("CCA improvement")
- Kornblith, Simon, et al. ["Similarity of neural network representations revisited."](https://arxiv.org/abs/1905.00414) *arXiv preprint arXiv:1905.00414*. 2019.

### Net expressivity
- Hanin, Boris, and David Rolnick. ["Complexity of Linear Regions in Deep Networks."](https://arxiv.org/abs/1901.09021) *arXiv preprint arXiv:1901.09021*. 2019.

### Convergence guarantees
- Zhang, Guodong, James Martens, and Roger Grosse. ["Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks."](https://arxiv.org/abs/1905.10961) *arXiv preprint arXiv:1905.10961*. 2019.

### Architecture and initialization
- Hanin, Boris, and David Rolnick. ["How to start training: The effect of initialization and architecture."](https://arxiv.org/abs/1803.01719) *Advances in Neural Information Processing Systems.* 2018.


# Non-papers (blog posts, articles, & cetera)

- [Stochastic Weight Averaging and the Ornstein-Uhlenbeck Process](https://armenag.com/2019/05/13/stochastic-weight-averaging/) (ArmenAg blog post, May 2019)

- [Landscape Connectivity of Low Cost Solutions for Multilayer Nets](http://www.offconvex.org/2019/06/16/modeconnectivity/) (Rong Ge blog post, Jun 16, 2019)

